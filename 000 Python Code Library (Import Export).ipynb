{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collection of all common python codes useful for Import and Export of various common data sources.\n",
    "\n",
    "Created by: Grace Choo\n",
    "\n",
    "Date of Creation: 19 September 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Zen of Python, by Tim Peters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful is better than ugly.\n",
    "\n",
    "Explicit is better than implicit.\n",
    "\n",
    "Simple is better than complex.\n",
    "Complex is better than complicated.\n",
    "\n",
    "Flat is better than nested.\n",
    "Sparse is better than dense.\n",
    "\n",
    "Readability counts.\n",
    "\n",
    "Special cases aren't special enough to break the rules.\n",
    "\n",
    "Although practicality beats purity.\n",
    "Errors should never pass silently.\n",
    "Unless explicitly silenced.\n",
    "\n",
    "In the face of ambiguity, refuse the temptation to guess.\n",
    "\n",
    "There should be one-- and preferably only one --obvious way to do it.\n",
    "Although that way may not be obvious at first unless you're Dutch.\n",
    "\n",
    "Now is better than never.\n",
    "Although never is often better than *right* now.\n",
    "\n",
    "If the implementation is hard to explain, it's a bad idea.\n",
    "If the implementation is easy to explain, it may be a good idea.\n",
    "\n",
    "Namespaces are one honking great idea -- let's do more of those!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import only single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename=r\"C:\\Users\\Documents\\Downloads\\titanic.txt\"\n",
    "data=pd.read_csv(filenamename, sep='^')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Multiple Files and append into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First create a function/macro\n",
    "def F_findFile(pattern, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if fnmatch.fnmatch(name, pattern):\n",
    "                result.append(os.path.join(name))\n",
    "    return result\n",
    "\n",
    "FilePath = r\"C:\\Users\\Documents\\Downloads\"\n",
    "\n",
    "#import all cancel log file\n",
    "ListofFiles = []\n",
    "ListofFiles = F_findFile('POL_NONMOTOR_*.txt', FilePath)\n",
    "\n",
    "\n",
    "for file in ListofFiles:\n",
    "    data = pd.read_csv(FilePath + \"\\\\\" + file, sep='^')\n",
    "    AllFile.append(data)\n",
    "\n",
    "#combine all imported data into 1 table\n",
    "AllFile = pd.concat(AllFile)\n",
    "\n",
    "## import data end ##\n",
    "\n",
    "\n",
    "## additional things that can be useful ##\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame: data and no header\n",
    "data=pd.read_csv(file,header=None,nrows=5)\n",
    "\n",
    "\n",
    "#remove all duplicates\n",
    "AllFile = AllFile.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pickled Files\n",
    "- Use this if you want to safe smaller files. More efficient than csv but less efficient than Parquet Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pickled_fruit.pkl','rb') as file:\n",
    "    data = pickle.load(file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Parquet Files\n",
    "- use this if you have to save large dataset and for space and time efficiency\n",
    "- need to install a package called \"fastparquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to read only specific columns\n",
    "df = pd.read_parquet('test.parquet', columns=['COL1','COL2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SAS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SQL Table (works only on MS Azure Databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from icekachangz server, to_sql method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
    "curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\n",
    "apt-get update\n",
    "ACCEPT_EULA=Y apt-get install msodbcsql17\n",
    "apt-get -y install unixodbc-dev\n",
    "sudo apt-get install python3-pip -y\n",
    "pip3 install --upgrade pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.put(\"/databricks/init/<YourClusterName>/pyodbc-install.sh\",\"\"\"\n",
    "#!/bin/bash\n",
    "sudo apt-get update\n",
    "sudo apt-get -q -y install unixodbc unixodbc-dev\n",
    "sudo apt-get -q -y install python3-dev\n",
    "/databricks/python/bin/pip install pyodbc\n",
    "\"\"\", True)\n",
    "for driver in pyodbc.drivers():\n",
    "    print(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can get these info from Connection strings under ODBC of your desired sql database.\n",
    "server = \"{Insert your SQL Server here}\"                #SQL Server\n",
    "port = \"1433\"\n",
    "database = \"{Insert your Database here}\"                #SQL Database\n",
    "username = \"{Insert your Username here}\"                #Username\n",
    "password = \"{Insert your password here}\"                #Password\n",
    "\n",
    "cnxn_str = \"DRIVER=\"+driver+\";SERVER=\"+server+\",\"+port+\";Database=\"+database+\";Uid=\"+username+\";Pwd=\"+password+\";\"\n",
    "cnxn = pyodbc.connect(cnxn_str)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql(\"SELECT TOP(100) * FROM {Insert your data table here}\", cnxn)\n",
    "\n",
    "#Check table\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import from icekachangz server, spark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can get these info from Connection strings under JDBC of your desired sql database.\n",
    "jdbcHostname = \"{Insert your SQL Server here}\"                #SQL Server\n",
    "jdbcPort = 1433\n",
    "jdbcDatabase = \"{Insert your Database here}\"                #SQL Database\n",
    "jdbcUsername = \"{Insert your Username here}\"                #Username\n",
    "jdbcPassword = \"{Insert your password here}\"                #Password\n",
    "\n",
    "\n",
    "#Create the JDBC URL without passing in the user and password parameters. (no need to change anything here)\n",
    "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};database={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkDF is a spark table\n",
    "SparkDF = spark.read.format(\"jdbc\").option(\"url\", jdbcUrl).option(\"dbtable\",\"{Insert Your Table here}\").load()\n",
    "display(SparkDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SQL Table (Can be used in jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import os, fnmatch\n",
    "\n",
    "server = \"{Insert your SQL Server here}\"\n",
    "database = \"{Insert your Database here}\"\n",
    "\n",
    "#Connection String\n",
    "connection = pyodbc.connect('DRIVER={SQL Server Native Client 11.0};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "SQLQuery = \"select * from {Insert Your Table here}\"\n",
    "df = pd.read_sql_query(SQLQuery, connection)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DBF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dbfread import DBF\n",
    "\n",
    "#import data in this location if CutMonth has value of more than 9\n",
    "dbfFile = r\"C:\\Users\\Documents\\Downloads\\{Insert your dbf file name here}.DBF\"\n",
    "\n",
    "#Import Exposure DBF data as dataframe\n",
    "NewDS=DataFrame(iter(DBF(dbfFile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(r'Path where the Excel file is stored\\File name.xlsx', sheet_name='your Excel sheet name')\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to skip rows\n",
    "\n",
    "df = pd.read_excel(r'Path where the Excel file is stored\\File name.xlsx', sheet_name='your Excel sheet name', skiprows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_csv Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=r\"C:\\Users\\Documents\\Downloads\\titanic.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "#if got UnicodeEncodeError then use this:\n",
    "df.to_csv(filename, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=r\"C:\\Users\\Documents\\Downloads\\titanic.csv\"\n",
    "df.to_csv(filename, index=False, sep=\"^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=r\"C:\\Users\\Documents\\Downloads\\titanic.csv\"\n",
    "file=open(filename,mode='w') # where w is to write\n",
    "file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_excel method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These will replace existing files. proceed with caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=r\"C:\\Users\\Documents\\Downloads\\titanic.xlsx\"\n",
    "df.to_excel(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign sheet name:\n",
    "filename=r\"C:\\Users\\Documents\\Downloads\\titanic.xlsx\"\n",
    "df.to_excel(filename, index=False, header=True, sheet_name='Sheet_name_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to output new sheet without replacing other existing sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "filename=r\"C:\\Users\\Documents\\Downloads\\titanic.xlsx\"\n",
    "book = load_workbook(filename)\n",
    "writer = pd.ExcelWriter(filename, engine = 'openpyxl')\n",
    "writer.book = book\n",
    "Combine_all_List.to_excel(writer, index=False, sheet_name = 'DataListing')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pickle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(Path_Location + \"yourtablename.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Parque Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(Path_Location + \"yourtablename.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export tables into SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Database Username and Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can get these info from Connection strings under ODBC of your desired sql database.\n",
    "#driver = \"{ODBC Driver 17 for SQL Server}\"\n",
    "server = \"{Insert your SQL Server here}\"                #SQL Server\n",
    "port = \"1433\"\n",
    "database = \"{Insert your Database here}\"                #SQL Database\n",
    "username = \"{Insert your username here}\"                #Username\n",
    "password = \"{Insert your password here}\"                #Password\n",
    "\n",
    "# to check for drivers\n",
    "for driver in pyodbc.drivers():\n",
    "    print(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odbc_str = 'DRIVER=' + driver + ';SERVER=' + server + ';PORT=1433;UID=' + username + ';DATABASE=' + database + ';PWD=' + password\n",
    "connect_str = 'mssql+pyodbc:///?odbc_connect=' + urllib.parse.quote_plus(odbc_str)\n",
    "engine = create_engine(connect_str)\n",
    "\n",
    "def to_sql(df, table):\n",
    "    df.to_sql(table, engine, if_exists = \"append\", index=False, chunksize = 100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excecute export table to target sql database\n",
    "to_sql(POL_MOTOR, \"POL_MOTOR\")\n",
    "to_sql(POL_NONMOTOR, \"POL_NONMOTOR\")\n",
    "to_sql(POL_LIFE, \"POL_LIFE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Windows Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "#You can get these info from Connection strings under ODBC of your desired sql database.\n",
    "server = \"{Insert your SQL Server here}\"                #SQL Server\n",
    "database = \"{Insert your Database here}\" \n",
    "\n",
    "odbc_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;'\n",
    "connect_str = 'mssql+pyodbc:///?odbc_connect=' + urllib.parse.quote_plus(odbc_str)\n",
    "engine = create_engine(connect_str)\n",
    "def to_sql(df, table):\n",
    "    df.to_sql(table, engine, if_exists = \"replace\", index=False, chunksize = 100)\n",
    "\n",
    "\n",
    "#Excecute export table to target sql database\n",
    "to_sql(Mail03, \"FACT_MYA_EMAIL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "301.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
